\chapter{Introduction}
\label{chap:intro}

The Transformer Architecture \cite{attention} has improved the state-of-the-art in various sequence modeling tasks. Transformer-based Large Language Models (LLM), such as BERT \cite{bert}, GPT \cite{gpt}, and LLaMA \cite{llama,llama2}, with billions of parameters, have displayed near-human or sometimes even superhuman performance on specific tasks. The capability to process and generate coherent and contextually relevant text has led to their widespread adoption. However, Transformer's and LLM's high efficiency comes at the cost of a quadratic computational and memory complexity concerning the sequence length, so deploying such models in real-world scenarios can be financially and environmentally costly.

In the realm of research and development (R\&D), expansive interest focuses on applying LLM in production environments, emphasizing computational efficiency. This thesis includes exploring methods such as model distillation and other techniques to reduce model size and complexity, thereby making them more practical and sustainable for real-world applications. Previous research focused mainly on experiments on distilling models in general, and last year â€“ prompt-based distillation technique for the LLM\@. However, the unique challenges posed by the complexity of LLM require distillation techniques that still need to be explored.

In this research, I create the framework to distill State-Of-The-Art LLM on question-answering datasets; and using it distill models such as LLaMA, Mistral \cite{mistral} to their small fellows, such as T5 \cite{t5} and GPT-2.

The paper is structured as follows. Chapter 2 presents some related work about making ML models and especially LLMs smaller and ready to deploy. Chapter 3 describes the main distillation methodology. Chapter 4 deals with the experimental set-up, in particular the change metrics proposed in this research. The respective results and discussions are presented in Chapter 5. Finally, Chapter 6 presents conclusions.
