\chapter{Introduction}
\label{chap:intro}

The Transformer Architecture has improved the state-of-the-art in various sequence modeling tasks \cite{attention}. Transformer-based Large Language Models (LLM), such as GPT \cite{gpt}, with billions of parameters, have displayed near-human or sometimes even superhuman performance on specific tasks. The capability to process and generate coherent and contextually relevant text has led to their widespread adoption. However, Transformer's and LLM's high efficiency comes at the cost of a quadratic computational and memory complexity concerning the sequence length, so deploying such models in real-world scenarios can be financially and environmentally costly.

In the realm of research and development, expansive interest focuses on applying LLM in production environments, emphasizing computational efficiency. This thesis explores knowledge distillation (KD) methods and other techniques to reduce model size and complexity, making them more practical and sustainable for real-world applications.

In this study, I mainly consider the comparative analysis and application of KD techniques in the field of LLM\@. The central thesis revolves around understanding how these KD methods can be leveraged to optimize LLMs, making them more viable for practical applications and minimizing the degradation of their capabilities.

In chemistry, distillation refers to separating the components of a liquid mixture consisting of two or more distinct substances. This separation process is achieved by selectively boiling the mixture and subsequently condensing the vapors in a distillation apparatus, commonly known as a still. This technique leverages differences in the boiling points of the substances to effect their separation, allowing each component to be isolated based on its specific boiling temperature \cite{chem_distil}.

Transitioning from the concept of chemical distillation to machine learning (ML) knowledge distillation, we observe a thematic shift from physical separation processes to the abstraction of knowledge transfer within computational models. In ML, particularly in the context of KD, the process is metaphorically similar but functionally distinct. KD involves transferring knowledge from a larger, more complex model to a smaller, more efficient one. The `boiling down' of knowledge occurs not through heat but through training processes where the student model learns to mimic the performance of the teacher model. This is achieved by optimizing the student model to reproduce the output distributions of the teacher model, effectively condensing the expansive knowledge of the teacher into a more compact form suitable for efficient deployment \cite{distilling}.

While both processes differ vastly in their mechanisms — physical versus computational — they share a core principle of extracting and isolating essential features from a broader set. In chemistry, this is the individual components of a mixture; in machine learning, it is the critical information and predictive capabilities contained within large datasets and complex model architectures.

KD techniques are mainly categorized into white-box and black-box approaches. \textbf{White-box KD} \cite{distilling}, where the internal parameters of the teacher model are accessible like LLaMA \cite{llama,llama2} and Mistral \cite{mistral}, is becoming more valuable for the research community and industry, as it enables student models to receive more comprehensive training signals from the teacher models, potentially leading to improved performance. Additionally, it is important to note that in the past six months, some open-source LLMs have been released that either match or surpass the performance of GPT-3.5 across various benchmarks. These emerging models are noteworthy for their competitive performance levels and the accessibility they offer to their parameters and hidden states, making them ideal candidates for white-box KD research. Notably, while white-box KD has been extensively applied to classification models and smaller language understanding models, its application to generative LLMs is not well-studied. Consequently, much additional work is required before a complete understanding can be reached. For more detailed information about white-box KD, including the latest research developments in this area, refer to \autoref{section:whitebox}.

On the other hand, \textbf{Black-box KD} (data-free knowledge distillation \cite{dfkd}, prompt-based distillation) involves using only the output predictions of teacher models. This method has shown effectiveness in refining smaller models by training them on response pairs generated by LLM API\footnote{application programming interface}, like GPT-3.5 and GPT-4.

The primary objective of this research is to investigate the process of distilling an LLM into a text classification model and to evaluate the efficacy of this methodology in comparison with alternative approaches. The study focuses on black-box methods and includes a proposal for a novel distillation pipeline that combines the generative capabilities of sequence-to-sequence (Seq2Seq) models with the classification strengths of state-of-the-art encoder models.

The thesis is structured as follows. \autoref{chap:lr} presents some related work about knowledge distillation in ML models, highlighting recent advancements in the field of LLM\@. \autoref{chap:met} and \autoref{chap:impl} deal with the experimental setup, methodology of measuring the distillation effectiveness, experimental procedure description, and results. The respective analysis, discussions, and further work are presented in \autoref{chap:eval}. Finally, \autoref{chap:conclusion} presents conclusions of the entire work.

