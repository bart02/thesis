\chapter{Introduction}
\label{chap:intro}

The Transformer Architecture has improved the state-of-the-art in various sequence modeling tasks \cite{attention}. Transformer-based Large Language Models (LLM), such as GPT \cite{gpt}, and LLaMA \cite{llama,llama2}, with billions of parameters, have displayed near-human or sometimes even superhuman performance on specific tasks. The capability to process and generate coherent and contextually relevant text has led to their widespread adoption. However, Transformer's and LLM's high efficiency comes at the cost of a quadratic computational and memory complexity concerning the sequence length, so deploying such models in real-world scenarios can be financially and environmentally costly.

In the realm of research and development (R\&D), expansive interest focuses on applying LLM in production environments, emphasizing computational efficiency. This thesis includes exploring methods such as model knowledge distillation (KD) and other techniques to reduce model size and complexity, thereby making them more practical and sustainable for real-world applications.

In this paper, I mainly consider the comparative analysis and application of KD techniques in the field of LLM\@. The central thesis revolves around understanding how these KD methods can be leveraged to optimize LLMs, making them more viable for practical applications and minimize the degradation their capabilities.

KD techniques are mainly categorized into black-box and white-box approaches. Black-box KD (data-free knowledge distillation \cite{dfkd}, prompt-based distillation) involves using only the output predictions of teacher models, like GPT-3.5 and GPT-4. This method has shown significant effectiveness in refining smaller models by training them on response pairs generated by LLM APIs. On the other hand, white-box KD \cite{distilling}, where the internal parameters of the teacher model are accessible (\textit{e.g.} LLaMA and Mistral \cite{mistral}), is becoming more valuable for the research community and industry, as it enables student models to receive more comprehensive training signals from the teacher models, potentially leading to improved performance. Notably, while white-box KD has been extensively applied to classification models and smaller language understanding models, its application in generative LLMs remains relatively under-explored.

Additionally, it is important to note that in the past six months, there have been releases of some open-source LLMs that either match or surpass the performance of GPT-3.5 across various benchmarks. These emerging models are noteworthy not only for their competitive performance levels but also for the accessibility they offer to their parameters and hidden states, making them ideal candidates for white-box KD research.

In this research, I create the framework which will allow to easilly distill LLM with different techniques. The primary objectives of this framework are to streamline the distillation process, making it more accessible and efficient, to enable experimentation with a wide range of KD methods (including both black-box and white-box), and assess the performance of distilled models. Utilizing this framework, I aim to conduct comparative analyses between various KD techniques, focusing on their effectiveness in reducing model size and complexity while maintaining its performance.

The paper is structured as follows. Chapter \ref{chap:lr} presents some related work about making ML models and especially LLMs smaller and ready to deploy, also it presents all used in this research KD techniques. Chapter \ref{chap:met} and \ref{chap:impl} deals with the experimental set-up, methodology of measure the deistillation efectiveness, and experimental procedure description. The respective results and discussions are presented in Chapter \ref{chap:eval}. Finally, Chapter \ref{chap:conclusion} presents conclusions and further work.
