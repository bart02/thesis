@inproceedings{attention,
  title     = {Attention is All you Need},
  volume    = {30},
  url       = {https://papers.nips.cc/paper/7181-attention-is-all-you-need},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  year      = {2017}
}

@inproceedings{bert,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
  volume    = {1},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186}
}

@inproceedings{gpt,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {http://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  volume    = {33},
  year      = {2020}
}

@misc{llama,
  title         = {{LLaMA}: Open and Efficient Foundation Language Models},
  author        = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  year          = {2023},
  eprint        = {2302.13971},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{llama2,
  title         = {{Llama 2}: Open Foundation and Fine-Tuned Chat Models},
  author        = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  year          = {2023},
  eprint        = {2307.09288},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{mistral,
  title         = {{Mistral 7B}},
  author        = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
  year          = {2023},
  eprint        = {2310.06825},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  issn    = {1532-4435},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@misc{t0,
  title         = {Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author        = {Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Tali Bers and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M. Rush},
  year          = {2022},
  eprint        = {2110.08207},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{compressingllm,
  title         = {Compressing {LLMs}: The Truth is Rarely Pure and Never Simple},
  author        = {Ajay Jaiswal and Zhe Gan and Xianzhi Du and Bowen Zhang and Zhangyang Wang and Yinfei Yang},
  year          = {2023},
  eprint        = {2310.01382},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{distilling,
  title         = {Distilling the Knowledge in a Neural Network},
  author        = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
  year          = {2015},
  eprint        = {1503.02531},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@misc{distilbert,
  title         = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
  author        = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  year          = {2020},
  eprint        = {1910.01108},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{kl,
  title        = {On Information and Sufficiency},
  volume       = {22},
  issn         = {0003-4851},
  doi          = {10.1214/aoms/1177729694},
  pages        = {79--86},
  number       = {1},
  journaltitle = {The Annals of Mathematical Statistics},
  shortjournal = {Ann. Math. Statist.},
  author       = {Kullback, S. and Leibler, R. A.},
  date         = {1951-03},
  langid       = {english}
}


@inproceedings{multidistil,
  author    = {Yang, Ze and Shou, Linjun and Gong, Ming and Lin, Wutao and Jiang, Daxin},
  title     = {Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System},
  year      = {2020},
  isbn      = {9781450368223},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  doi       = {10.1145/3336191.3371792},
  booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
  pages     = {690–698},
  numpages  = {9},
  keywords  = {two-stage, multi-teacher, model compression, knowledge distillation, distillation pre-training},
  location  = {Houston, TX, USA},
  series    = {WSDM '20}
}

@inproceedings{stepbystep,
  title     = {Distilling {Step-by-Step!} Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes},
  author    = {Hsieh, Cheng-Yu  and
               Li, Chun-Liang  and
               Yeh, Chih-kuan  and
               Nakhost, Hootan  and
               Fujii, Yasuhisa  and
               Ratner, Alex  and
               Krishna, Ranjay  and
               Lee, Chen-Yu  and
               Pfister, Tomas},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2023.findings-acl.507},
  pages     = {8003--8017}
}

@misc{cot,
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author        = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
  year          = {2023},
  eprint        = {2201.11903},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{socraticcot,
  title     = {Distilling Reasoning Capabilities into Smaller Language Models},
  author    = {Shridhar, Kumar  and
               Stolfo, Alessandro  and
               Sachan, Mrinmaya},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2023.findings-acl.441},
  pages     = {7059--7073}
}

@inproceedings{dfkd,
  author    = {Micaelli, Paul and Storkey, Amos J},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Zero-shot Knowledge Transfer via Adversarial Belief Matching},
  url       = {http://papers.nips.cc/paper/9151-zero-shot-knowledge-transfer-via-adversarial-belief-matching},
  volume    = {32},
  year      = {2019}
}

@inproceedings{anli,
  title     = {Adversarial {NLI}: A New Benchmark for Natural Language Understanding},
  author    = {Nie Yixin and Williams Adina and Dinan Emily and Bansal Mohit and Weston Jason and Kiela Douwe},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2020.acl-main.441},
  pages     = {4885--4901}
}

@inproceedings{cose,
  title     = {{Explain Yourself! Leveraging} Language Models for Commonsense Reasoning},
  author    = {Rajani, Nazneen Fatema  and
               McCann, Bryan  and
               Xiong, Caiming  and
               Socher, Richard},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/P19-1487},
  pages     = {4932--4942}
}

@inproceedings{esnli,
  author    = {Camburu, Oana-Maria and Rockt\"{a}schel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  title     = {{e-SNLI}: Natural Language Inference with Natural Language Explanations},
  url       = {http://papers.nips.cc/paper/8163-e-snli-natural-language-inference-with-natural-language-explanations},
  volume    = {31},
  year      = {2018}
}

@inproceedings{snli,
  title     = {A large annotated corpus for learning natural language inference},
  author    = {Bowman, Samuel R.  and
               Angeli, Gabor  and
               Potts, Christopher  and
               Manning, Christopher D.},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  month     = sep,
  year      = {2015},
  address   = {Lisbon, Portugal},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/D15-1075},
  pages     = {632--642}
}

@inproceedings{svamp,
  title     = {Are {NLP} Models really able to Solve Simple Math Word Problems?},
  author    = {Patel, Arkil  and
               Bhattamishra, Satwik  and
               Goyal, Navin},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = jun,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2021.naacl-main.168},
  pages     = {2080--2094}
}

@online{zeroshotclf,
  title        = {What is Zero-Shot Classification?},
  url          = {https://huggingface.co/tasks/zero-shot-classification},
  month        = nov,
  year         = {2022},
  organization = {Hugging Face, Inc},
  urldate      = {2024-03-18}
}

@online{kd_simplified,
  title   = {Knowledge Distillation : Simplified},
  url     = {https://towardsdatascience.com/knowledge-distillation-simplified-dd4973dbc764},
  author  = {Ganesh, Prakhar},
  urldate = {2024-04-25},
  date    = {2019-10-07}
}

@article{montecarlo,
  issn      = {01621459},
  author    = {Nicholas Metropolis and S. Ulam},
  journal   = {Journal of the American Statistical Association},
  number    = {247},
  pages     = {335--341},
  publisher = {American Statistical Association, Taylor & Francis, Ltd.},
  doi       = {10.2307/2280232},
  title     = {{The Monte Carlo method}},
  volume    = {44},
  month     = sep,
  year      = {1949}
}

@misc{minillm,
  title         = {{MiniLLM}: Knowledge Distillation of Large Language Models},
  author        = {Yuxian Gu and Li Dong and Furu Wei and Minlie Huang},
  year          = {2024},
  eprint        = {2306.08543},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{policy_gradient,
  author    = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {1057--1063},
  publisher = {MIT Press},
  title     = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  url       = {https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation},
  volume    = {12},
  year      = {1999}
}

@inproceedings{slim,
  title     = {For Distillation, Tokens Are Not All You Need},
  author    = {Mrigank Raman and Pranav Mani and Davis Liang and Zachary Lipton},
  booktitle = {NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
  year      = {2023},
  url       = {https://openreview.net/forum?id=2fc5GOPYip}
}

@misc{tinyllm,
  title         = {{TinyLLM}: Learning a Small Student from Multiple Large Language Models},
  author        = {Yijun Tian and Yikun Han and Xiusi Chen and Wei Wang and Nitesh V. Chawla},
  year          = {2024},
  eprint        = {2402.04616},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@book{chem_distil,
  title      = {Distillation: Principles and Practice},
  isbn       = {9781119414698},
  shorttitle = {Distillation},
  publisher  = {John Wiley \& Sons},
  author     = {Stichlmair, Johann G. and Klein, Harald and Rehfeldt, Sebastian},
  date       = {2021-05-07},
  langid     = {english},
  doi        = {10.1002/9781119414674}
}

@inproceedings{wos,
  title     = {{HDLTex}: Hierarchical Deep Learning for Text Classification},
  doi       = {10.1109/icmla.2017.0-134},
  booktitle = {2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)},
  publisher = {IEEE},
  author    = {Kowsari, Kamran and Brown, Donald E. and Heidarysafa, Mojtaba and Jafari Meimandi, Kiana and Gerber, Matthew S. and Barnes, Laura E.},
  year      = {2017},
  month     = dec
}

@article{palm,
  author  = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sashank and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  title   = {{PaLM}: scaling language modeling with pathways},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {240},
  pages   = {1--113},
  issn    = {1532-4435},
  url     = {http://jmlr.org/papers/v24/22-1144.html}
}